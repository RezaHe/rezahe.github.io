import "./projectBodies.css";
import Zoom from 'react-medium-image-zoom'
import 'react-medium-image-zoom/dist/styles.css'

const ProjectBody01 = () => {
    return (
        <div>
            <em style={{fontSize: "15px"}}>Last updated: 17 Nov, 2023</em>
            <div style={{margin: "0 auto", textAlign: "center"}}>
                <figure>
                    <Zoom>
                        <img
                            alt="SpartanX Logo"
                            src="../projects_spartan_x_01.jpg"
                            width="200"
                        />
                    </Zoom>
                    <figcaption style={{fontSize: "15px"}}><em>
                        SpartanX
                    </em></figcaption>
                </figure>
            </div>

            “Is it possible?” This was the first question every person asked me when I explained what we do in our
            startup. At times, I felt hopeless about the results of our work. However, the primary reason we persevered
            in our struggle was the realization that major companies, such as <a target="_blank" rel="noreferrer"
                                                                                 href="https://www.rentec.com/Home.action?index=true">RenTech</a> hedge
            fund and <a target="_blank" rel="noreferrer"
                        href="https://www.blackrock.com/corporate">BlackRock</a> investment management, have
            successfully employed similar approaches, serving as examples of success.
            <br/><br/>
            In Cardano Trader, our goal was to create a fully automated system for managing assets in the Tehran Market
            using AI, particularly ML and DL methods. I dedicated two and a half years to this endeavor, collaborating
            with two friends who joined me later. We worked day and night on this project. Now, with two years having
            passed since the failure of our startup and the expiration of our NDA, I aim to share some insights about
            our journey, highlighting both the challenges and successes we experienced in implementing this idea in the
            real world.
            <br/><br/>
            The system comprises three main components: data listener, an AI core, and an accounts manager.
            The data listener component functioned as a scraper, collecting technical data primarily from two
            sources—the market's official page and the broker's page. This part's responsibility was to publish data in
            memory, and to minimize delays, the data was directly written to <a target="_blank" rel="noreferrer"
                                                                                href="https://www.softprayog.in/programming/interprocess-communication-using-system-v-shared-memory-in-linux#:~:text=Shared%20memory%20is%20one%20of,the%20message%20queues%20and%20semaphores.">shared
            memory</a>. The
            AI core served as the hub for various algorithms employing different strategies. These
            algorithms determined, at each time frame, the selection of symbols (where we could invest our money in
            different symbols in the market) and the portion of the entire asset to be allocated. We implemented three
            various algorithms for this segment, with SpartanX, representing a fine-tuned version of
            Spartan, being one of them. Lastly, the accounts manager, also a scraper, managed orders generated by each
            algorithm in the AI core,
            facilitating their submission, modification, or cancellation on the exchange page for multiple accounts.
            <br/><br/>
            In this article, I aim to elucidate SpartanX and the concept behind it. I won’t delve into the other
            components and algorithms, as they warrant separate articles. Initially, I will delve into the origin of
            the SpartanX idea by presenting results from its precursor concepts. Then, I will provide a detailed
            explanation. Finally, I will present real-world test results and conclude the article by addressing
            practical issues.
            <br/><br/>
            <div className="heading-1">Before Going Deeper</div>
            <br/><br/>
            It is worthwhile to mention that in real-world applications, there are many things you need to do beyond
            just loading data and training a model. Consider the following aspects to be addressed in a full data
            science project: gathering and cleaning data, reconstructing missing data, hardware requirements,
            versioning and checkpointing the models, visualization, and validation, real-time monitoring, testing, and
            retraining based on test results.
            <br/><br/>
            <div className="heading-1">About Tehran Market</div>
            <br/><br/>
            The Tehran Stock Exchange is Iran’s largest stock market, with an estimated daily transaction value of about
            $120 million. At the time we developed our system, there were approximately 750 active companies, and their
            shares could be traded as symbols.
            <br/><br/>
            The market has some restrictions and rules that pose challenges for a data scientist. For example, the
            market opens at 9 AM and closes at 12:30 PM, although the opening hours have occasionally changed.
            Additionally, each day, each symbol must fluctuate within a pre-determined range based on yesterday's
            closing price. This range varies for different symbols. Moreover, trading for each symbol must halt due to
            regulatory requirements, leading to the blocking of all assets. Designing a system must take these scenarios
            into consideration.
            <br/><br/>
            <div style={{margin: "0 auto", textAlign: "center"}}>
                <figure>
                    <Zoom>
                        <img
                            alt="SpartanX Logo"
                            src="../projects_spartan_x_02.jpg"
                            width="100%"
                        />
                    </Zoom>
                    <figcaption style={{fontSize: "15px"}}><em>
                        The price of the symbol DELOR in Rials (the currency of Iran) is depicted from mid-April to
                        mid-October 2020. The price is represented by dots for different days, with colors indicating
                        positive/negative trends—red, white, or green. White rectangles indicate the price limits for
                        each day. The red and grey vertical areas represent days when the symbol's trade is restricted.
                    </em></figcaption>
                </figure>
            </div>
            <br/><br/>
            <div className="heading-1">Start: Prediction of the Future of Each Symbol</div>
            <br/><br/>
            The initial idea involves predicting the future price of each symbol and subsequently selecting the best
            symbols based on both their anticipated future and the confidence level of these predictions. For instance,
            if we estimate that symbol X's price will increase by 5% in 7 days with a confidence level of 70%, and
            symbol Y's price will rise by 3.5% in 7 days with a confidence level of 80%, we can allocate our assets
            among these two symbols according to our risk threshold. Another consideration is reserving a portion of our
            assets for potential future opportunities, a decision that is intricate and will be discussed later.
            <br/><br/>
            At the start of our startup, we trained various regression models and networks to predict the future price
            of each symbol. However, we encountered several challenges:
            <br/>
            <ul>
                <li>
                    Determining the prediction horizon: How many future points should be considered—next minute, next
                    hour, next day, next week, or next month?
                </li>
                <li>
                    Measuring prediction confidence: How can we quantify the confidence of our predictions?
                </li>
                <li>
                    Feature selection: What data should be included as features? Should additional economic measures
                    like MACD, EMA, and RSI be used?
                </li>
                <li>
                    Model training: Should a single model be trained for all symbols, or is it more effective to use
                    separate models for different symbols?
                </li>
                <li>
                    Non-stationary price history: The price history is non-stationary and varies over time and among
                    symbols. How can we address this variability?
                </li>
            </ul>
            <br/>
            Initially, we aggregated all symbols throughout their entire history to create a comprehensive dataset.
            Various features, such as MACD, were computed for each price and incorporated into our dataset. To address
            the non-stationary problem, we implemented a moving window of a specified size and utilized the min-max
            normalization method to maintain the price range within a specific interval. As for the model's response, we
            calculated the relative price change, representing the change in future price relative to the current price.
            <br/><br/>
            Despite the involvement of all symbols and a dataset size of approximately 200,000 samples, the abundance of
            features posed the curse of dimensionality problem for the model. To address this, we initially employed a
            Random Forest model to identify the most informative features, ultimately narrowing them down to about 20
            key features.
            <br/><br/>
            Over more than two months, we trained various models to predict future prices, but the results were
            consistently discouraging: almost always yielding predictions of zero. Regardless of changing both the model
            architecture and the dataset, the predictions remained stuck at zero. We explored a range of models,
            including RNN, LSTM, GRU, 1-dimensional CNN, Gaussian Process regression, and even Transformer.
            Additionally, we experimented with different selections of economic measures as features, various
            normalization methods, diverse time horizons from hours to weeks, and varied selections of data related to
            different dates in the market.
            <br/><br/>
            <div className="heading-1">What is the Meaning of Zero?</div>
            <br/><br/>
            We arrived at two possible explanations for our results. First, it might be a flawed assumption to aggregate
            all market data together. It is plausible that symbols' prices vary in distinct ways. For instance, Symbol
            X's price could rise under one specific condition, while under the same condition, the price of Y might
            drop. Moreover, this rule for a single symbol can change over time. The same condition that contributed to
            the rise in the price of Symbol Z two years ago may not have the same effect this year due to the overall
            market being in a harsh condition.
            <br/><br/>
            Secondly, we considered that the cause of price change within the time frame we examined might not be
            adequately represented in our features. For instance, if we aim to predict the price of Symbol Z in the next
            month, relying on daily trade value, current price, RSI, and similar features might be insufficient.
            Instead, the most significant determinant could be found in the net sales of the company. Unfortunately,
            thess types of fundamental data are not present in our dataset, and we do not have access to them.
            <br/><br/>
            <div className="heading-1">SpartanX</div>
            <br/><br/>
            So, we decided to shift our perspective and implemented three major changes. The first change involved
            time-dependent clustering of symbols. Our assumption was that the price dynamics of each symbol vary
            smoothly over time (with time referring to the day in this context). On any given day, all 750 symbols are
            grouped into 10 different clusters based on certain features extracted from their price fluctuations.
            <br/><br/>
            The second change involved approaching the problem as a classification problem rather than a regression one.
            In fact, accurately predicting the exact future price is challenging. When consulting an expert about
            a symbol, they often provide general directional insights, such as "It will go up" or "It will drop." In
            practice, the focus is more on understanding the overall trend rather than pinpointing the exact price.
            <br/><br/>
            Indeed, we labeled our data at each point as either a positive or negative sample. The label signifies that
            buying
            the share at that time point could result in a benefit. However, a challenge arises: At which point sell
            that share? How much benefit do you want to make and sell your share? Do you want to hold the share if the
            price drops, perhaps for a return? The classification of good/bad labels is highly dependent on the chosen
            trading strategy.
            <br/><br/>
            The third change involved reducing the prediction horizon to one hour. After thorough examination, I can
            confidently state that there is no substantial information in the technical data of the market for
            predicting the long-term future of a symbol, such as a month later. The market, being influenced by
            significant players, involves human decision-making based on several unpredictable factors. However, in a
            short time period, we observed that the price exhibits a discernible dynamic.
            <br/><br/>
            This entails a trade-off between profit and prediction ability. Lowering the future horizon allows for
            better price prediction, but short-term profits are typically modest. This is where the concept of
            High-Frequency Trading comes into play. In this method, numerous small trades are executed, and the
            cumulative result of these trades amounts to a significant profit.
            <br/><br/>
            Hence, SpartanX is divided into two interconnected sub-systems: the Signal Generator and the Stream Manager.
            The Signal Generator generates, every 2.5 seconds, a vector of probabilities for all symbols. Each entry in
            the vector represents the goodness of buying that symbol at that specific time (not predicting the future
            price
            value). The Stream Manager is responsible for managing the entire asset and utilizing it to invest in
            various symbols.
            <br/><br/>
            <div className="heading-1">Signal Generator</div>
            <br/><br/>
            At the beginning of each day, the Signal Generator extracts 5 features representing the price fluctuations
            of each symbol over the last 30 days. This results in a 5x30 matrix for each symbol. Then, we
            utilized <a target="_blank" rel="noreferrer" href="https://arxiv.org/abs/1802.03426">UMAP</a> to
            reduce these 150 features to 2 dimensions. Subsequently, we used <a target="_blank" rel="noreferrer"
                                                                                href="https://link.springer.com/chapter/10.1007/978-3-642-37456-2_14">HDBScan</a> to
            categorize all 750 symbols into 10 clusters based on their reduced 2 features. This process is repeated
            daily, assigning each symbol to one of the 10 clusters. It is worth noting that a symbol may be in a
            different cluster today than it was 20 days ago. To account for variations in UMAP reduction and HDBScan
            clustering, we employed <a target="_blank" rel="noreferrer"
                                       href="https://en.wikipedia.org/wiki/Procrustes_transformation">Procrustes</a> to
            align these features for consecutive days.
            <br/><br/>
            <div style={{margin: "0 auto", textAlign: "center"}}>
                <figure>
                    <Zoom>
                        <img
                            alt="Symbols Clustered"
                            src="../projects_spartan_x_03.gif"
                            width="100%"
                        />
                    </Zoom>
                    <figcaption style={{fontSize: "15px"}}><em>
                        Symbols are clustered, with each dot representing the reduced features of each symbol in 2
                        dimensions. Distinct colors correspond to each of the 10 clusters. The transition of clusters is
                        depicted in each frame of the animation, with dates annotated beneath the figure.
                    </em></figcaption>
                </figure>
            </div>
            <br/><br/>
            We found the 10 clusters to be intriguing as they exhibited a high sensitivity to related symbols. For
            instance, the symbol Khodro consistently appeared as the closest symbol to Vsapa on most days. This
            alignment is meaningful as both companies are prominent automobile manufacturers and share significant
            similarities. It is noteworthy that we didn't explicitly feed this knowledge to our model; rather, it
            emerged
            based on price variability, effectively identifying and recovering related symbols.
            <br/><br/>
            Examining the price of sample symbols across various clusters revealed a compelling pattern. In essence, our
            approach enabled us to segregate symbols based on their liquidity. Less liquid symbols exhibited more
            significant price jumps, while more liquid symbols proved resistant to large price changes due to the
            substantial support from high volumes of buy and sell orders. This separation not only provided valuable
            insights into market dynamics but also paved the way for tailored trading strategies, recognizing that
            trading in different symbols necessitates distinct approaches contingent on the liquidity of each symbol.
            <br/><br/>
            <div style={{margin: "0 auto", textAlign: "center"}}>
                <figure>
                    <Zoom>
                        <img
                            alt="Sample Price Clustered"
                            src="../projects_spartan_x_04.jpg"
                            width="100%"
                        />
                    </Zoom>
                    <figcaption style={{fontSize: "15px"}}><em>
                        Sample price changes for each cluster in a single day are illustrated, with the y-axis
                        representing the percentage change in price.
                    </em></figcaption>
                </figure>
            </div>
            <br/><br/>
            Then, we trained different models for these clusters, recognizing that the price dynamics differ
            among them. The model's output, as discussed earlier, signifies the desirability of buying. A higher
            probability suggests a potentially larger profit when buying shares of that symbol at that time. For
            labeling data, we established a simple trading strategy: we buy a share at time 't' with price 'p.' If any
            of the following criteria are met, we sell that share:
            <br/>
            <ol>
                <li>
                    If the current price falls by less than 2% of the maximum price in the time window from time 't' to
                    the present.
                </li>
                <li>
                    If the price falls by less than 1% of 'p.'
                </li>
                <li>
                    If the current time surpasses 1 hour from time 't.'
                </li>
            </ol>

            <br/>
            We iterated through all time points (sampled with a 2.5-second period) in the dataset for each day and apply
            these criteria. If the sell price is higher than the buy price plus the exchange fee, that time point is
            labeled as 'good.' Conversely, the remaining points are labeled as 'bad.'
            <br/><br/>
            By training our model in this manner, we ensured that if the model accuracy is high, and we follow these
            predictions along with the aforementioned trading strategy, we can benefit from the market.
            <br/><br/>
            We utilized a <a target="_blank" rel="noreferrer"
                             href="https://arxiv.org/abs/1706.03762">Transformer</a> network
            for this purpose due to its foundation on the attention mechanism. Unlike
            RNN, which struggles with inferring patterns occurring at different time scales, the Transformer, leveraging
            Multi-Head Attention blocks, effectively addresses this limitation. To tailor the Transformer for our
            problem, we made specific adjustments to the architecture, such as removing embedding and modifying
            positional encoding.
            <br/><br/>
            <div style={{margin: "0 auto", textAlign: "center"}}>
                <figure>
                    <Zoom>
                        <img
                            alt="Transformer Response"
                            src="../projects_spartan_x_05.jpg"
                            width="100%"
                        />
                    </Zoom>
                    <figcaption style={{fontSize: "15px"}}><em>
                        Sample predictions of the Transformer are depicted. The start of each red line signifies points
                        where the model predicted them as good places to invest (buy), while the end of the red line
                        indicates sell points according to the trade strategy.
                    </em></figcaption>
                </figure>
            </div>
            <br/><br/>
            <div className="heading-1">Stream Manager</div>
            <br/><br/>
            In an imaginary scenario where we have access to a perfect model that predicts the future of each symbol
            with high confidence, say 90%, determining the best approach for investment becomes nuanced. While one might
            suggest investing all assets in the share of a symbol the first time, there is a 10% chance it could be a
            bad
            decision leading to losses. Alternatively, waiting for a better opportunity to buy another symbol and
            potentially gain greater benefits is also a consideration. Adding to the complexity, if we already hold
            shares of symbol X and it is currently in a loss, the decision of whether to sell those shares to free up
            assets for a symbol with a higher probability introduces another layer of complexity to the investment
            strategy.
            <br/><br/>
            In my opinion and based on my experience, the selection of these strategies holds more significance than
            predicting the future price. This aspect is closely tied to economic topics such as risk management and
            asset management.
            <br/><br/>
            We devised a simple yet effective strategy for this. Initially, our entire asset, let's say 1, is divided
            into a specified number of parts, denoted as 'streams,' let's say N. In each trade, we utilize one of these
            streams to buy and sell shares. At each time point, the system examines all available streams. If one of
            them is available, the Stream Manager consults the Signal Generator for promising symbols to buy. If the
            probability of a symbol exceeds a threshold and that symbol has not been purchased before, the stream is
            allocated to that symbol. The Stream Manager continuously monitors all active streams, and, as previously
            outlined, if any of these criteria are met, the shares of that symbol are sold:
            <br/>
            <ol>
                <li>
                    If the current price falls by less than 2% of the maximum price in the time window from time 't' to
                    the present.
                </li>
                <li>
                    If the price falls by less than 1% of 'p.'
                </li>
                <li>
                    If the current time surpasses 1 hour from time 't.'
                </li>
            </ol>

            <br/>
            The parameters of this method play a crucial role in determining the overall benefit. For example, if the
            number N is increased, the profit will decrease since the total profit is divided by N; simultaneously, the
            risk will decline. Therefore, finding the optimal set of parameters is highly impactful and poses a
            challenging problem. We identified the best set through grid searching across all possible combinations over
            a 3-week period, using five powerful machines with a Core i9 CPU. The following image illustrates the
            impact of changing some of these parameters on the overall profit.
            <br/><br/>
            <div style={{margin: "0 auto", textAlign: "center"}}>
                <figure>
                    <Zoom>
                        <img
                            alt="Trading Parameters"
                            src="../projects_spartan_x_06.jpg"
                            width="100%"
                        />
                    </Zoom>
                    <figcaption style={{fontSize: "15px"}}><em>
                        The impact of varying distinct parameters of the trade strategy on daily profit is illustrated.
                        It is important to note that the profit can also be negative, emphasizing the potential
                        results of an incorrect choice of parameters, despite the Signal Generator perform well.
                    </em></figcaption>
                </figure>
            </div>
            <br/><br/>
            Recall the labeling mechanism in the Signal Generator part. It is important to note that labels are
            dependent
            on this parameter set. The best parameter set and the profit are also influenced by the output of the Signal
            Generator. Yet, we should fine-tune these two sub-systems together. Fine-tuning these two sub-systems
            together is a non-trivial task, and currently, we fine-tune Transformer models after identifying the best
            set.
            <br/><br/>
            <div className="heading-1">Results and Discussion</div>
            <br/><br/>
            The entire structure underwent multiple tests. In a test spanning approximately one and a half months with
            an initial asset of $100,000, we achieved a 15% gain. Notably, in one day, two streams were bought and sold
            in a symbol within just 6 seconds. However, upon further analysis, we discovered that this profit is heavily
            influenced by market circumstances. High-frequency trading necessitates high liquidity as orders must be
            executed swiftly. Additionally, this strategy performs well in neutral markets, exhibiting effectiveness
            neither in a bullish nor a bearish trend.
            <br/><br/>
            This strategy encountered two practical issues during real tests, resulting in a decline in overall
            performance. Firstly, buy orders often failed to complete, primarily due to liquidity issues. Secondly, as
            mentioned earlier, the Tehran exchange market's rule stipulates a specific price range. Consequently, during
            a downward trend, shareholders seek to sell their shares, leading to a lack of buyers and causing a price
            saturation with a lower bound. This, in turn, results in some streams being blocked for several days,
            preventing their use in other symbols for profit generation. These challenges significantly impacted the
            algorithm's performance.
            <br/><br/>
            As suggested earlier, the most crucial factor in this type of work is not finding the best machine to
            predict the future price; rather, it is more important to implement strategies and methods for
            decision-making based on the current situation. Reinforcement learning may offer a promising avenue. By
            defining agents capable of taking actions like creating and canceling orders, and modeling rewards as the
            profit from trading within a specific time period based on probabilities generated by the Signal Generator,
            a more adaptive approach could be achieved. Although we attempted to pursue this direction, time constraints
            hindered us from completing the task, ultimately leading to the collapse of our startup.
        </div>
    );
};


const ProjectBody02 = () => {
    return (
        <div>
            <em style={{fontSize: "15px"}}>Last updated: 19 Nov, 2023</em>
            <div style={{margin: "0 auto", textAlign: "center"}}>
                <figure>
                    <Zoom>
                        <img
                            alt="Image2Latex Logo"
                            src="../projects_image_to_latex_01.png"
                            width="100%"
                        />
                    </Zoom>
                    <figcaption style={{fontSize: "15px"}}><em>
                        Conversion of math image to LaTex code
                    </em></figcaption>
                </figure>
            </div>

            One of the most challenging aspects of writing an academic paper with LaTeX is inserting mathematical
            equations and formulas. One approach is to write the formula on a piece of paper, take a picture of it, and
            then use Optical Character Recognition (OCR) engines to convert it into a typed equation. While there are
            already OCR engines that can convert handwritten text into typed text, there are few examples that
            specialize in handling mathematical images.
            <br/><br/>
            In this article, I am going to explain how I utilized deep neural networks to convert a computer-generated
            image of a mathematical formula into LaTeX code. It is worth noting that handling handwritten images
            presents more challenges. I employed innovative methods for this task as part of my deep learning course
            project.
            <br/><br/>
            The main concept is attention. When writing a formula, the process involves reading from left to right,
            writing each token while maintaining focus on that token and its position within the entire formula. This
            approach is akin to the process of generating a caption for a figure. In a <a target="_blank"
                                                                                          rel="noreferrer"
                                                                                          href="https://arxiv.org/abs/1502.03044">specific
            method</a>, a
            CNN network extracts the feature map of an image, followed by the use
            of a language model with an attention mechanism. This model sequentially generates each token while
            selectively focusing on different locations within the feature map of the image.
            <br/><br/>
            <div style={{margin: "0 auto", textAlign: "center"}}>
                <figure>
                    <Zoom>
                        <img
                            alt="Show, Attend and Tell"
                            src="../projects_image_to_latex_02.png"
                            width="100%"
                        />
                    </Zoom>
                    <figcaption style={{fontSize: "15px"}}><em>
                        Show, Attend and Tell from https://zhuanlan.zhihu.com/p/32333802
                    </em></figcaption>
                </figure>
            </div>
            <br/><br/>
            In this approach, the provided image undergoes initial processing through a CNN network, extracting the
            image's feature map. This map serves as the foundation for the initial state of the RNN. At each state, the
            RNN generates the predicted output vocabulary and produces a map of the same size as the feature map. This
            map functions as the attention mechanism. The map is multiplied by the extracted feature map, and the result
            becomes the input for the subsequent RNN step. Consequently, in generating each vocabulary element, both the
            previous vocabulary and the attention result are fed into the RNN. The expectation is that, for each
            vocabulary element, the feature map adjusts the pixels corresponding to the relevant part of the image,
            effectively filtering out irrelevant areas.
            <br/><br/>
            While the idea is promising, there is speculation regarding the actual functioning of attention as
            anticipated. The absence of a guarantee that the attention map consistently selects the relevant part of the
            image for generating each output vocabulary raises concerns. Additionally, the entire structure is trained
            solely based on the loss computed at the output, without assurance that every aspect of the structure is
            effectively trained, especially considering the issue of gradient vanishing in RNNs. In response to these
            concerns, I have modified the aforementioned structure with the following suggestions:
            <br/>
            <ul>
                <li>
                    Utilize Inception modules as CNN blocks due to their capability to handle different sizes of various
                    tokens in the input image.
                </li>
                <li>
                    Pre-train the Inception modules in another task with specific labeling. Enforce the Inception
                    network to initially identify the bounding box of the entire given image, ensuring that the
                    attention mechanism aligns with expectations.
                </li>
                <li>
                    Apply a Transformer network with skip-gram embedding and the bounding box output of the Inception
                    network to translate math tokens (markups) into math characters using an attention mechanism.
                </li>
            </ul>
            <br/>
            <div className="heading-1">Data Exploration</div>
            <br/><br/>
            The dataset, consisting of nearly 140k samples of math images paired with LaTeX code, can be accessed
            through <a target="_blank" rel="noreferrer" href="https://untrix.github.io/i2l/140k_download.html">this
            link</a>.
            The LaTeX code corpus comprises around 550 unique vocabularies,
            such as &#123;, Q, ^, \mu, and \frac. While the vocabulary size may not be extensive,
            the challenge is more complex than initially anticipated. Some vocabularies are combined to form
            intricate markups, such as \begin&#123;array&#125;, \begin&#123;matrix&#125;,
            and \begin&#123;tabular&#125;.
            Additionally, certain vocabularies lack a corresponding markup as output, including \ref and \label.
            Some are positioned above, below, or in proximity to the next vocabulary,
            such as \sqrt, \widehat, \widetild, \matrix, \over, \fbox, \underline,
            and \choose.
            Lastly, there are symbols that influence the appearance of the succeeding vocabulary output, such
            as \mathrm, \mathsf, \mathcal, \mbox, \rm, \scriptscriptstyle, \sf, \textbf,
            and \textit.
            <br/><br/>
            <div className="heading-1">Unique Markups</div>
            <br/><br/>
            By examining each combination of distinct markups, particularly those mentioned in the previous section, I
            identified 2608 unique visual markups. To achieve this, I initially extracted all 550 unique vocabularies
            from the training labels. Subsequently, I checked each vocabulary in various examples to pinpoint those
            causing the above difficulties. Finally, I created a simple .tex file template and used a for loop to
            automatically compile the .tex document, saving the output images in a directory. The following image
            displays some samples.
            <br/><br/>
            <div style={{margin: "0 auto", textAlign: "center"}}>
                <figure>
                    <Zoom>
                        <img
                            alt="Example unique markups"
                            src="../projects_image_to_latex_03.jpg"
                            width="100%"
                        />
                    </Zoom>
                    <figcaption style={{fontSize: "15px"}}><em>
                        Example unique markups
                    </em></figcaption>
                </figure>
            </div>
            <br/><br/>
            <div className="heading-1">Finding Bounding Boxes</div>
            <br/><br/>
            After identifying all unique markups, I proceeded to determine the location of each markup and annotated it
            with a rectangular bounding box. This step is crucial for defining attention blocks. Indeed, what I did here
            is extracting a guide for the attention mechanism. I will delve into this in more detail later; for now,
            let's focus on the task of locating bounding boxes for all markups.
            <br/><br/>
            Traditional image processing methods, particularly the template matching approach, prove to be a valuable
            tool for this purpose. I utilized a well-known technique called template matching, which involves locating a
            specific pattern within a given image. In our case, the objective is to identify unique markups in the input
            image. The process entails sliding the unique markup across the image, calculating the correlation between
            the markup and each location it passes through, resulting in a correlation matrix. The peak index in this
            matrix corresponds to the location where the markup most closely resembles the pattern at that point. To
            enhance this process, I employed a more sophisticated method known as TM_SQDIFF_NORMED, which normalizes
            these correlations.
            <br/><br/>
            However, there are two challenges with this method. First, the size of the specific markup in the image may
            deviate from the original markup. Second, executing this method for all 2608 unique markups proves to be
            highly time-consuming. Fortunately, solutions exist to address these issues.
            <br/><br/>
            To address the issue of varying sizes, we can employ template matching with different sizes of the markup.
            Estimating the range of sizes involves comparing the size of the image with that of the markup.
            For the second problem, using the label of the image in the dataset (the LaTeX code) proves to be a useful
            strategy. Extracting the used markups and their frequency of occurrence can be easily achieved by reading
            the LaTeX code. It is important to be caution with specific vocabularies such as \mathcal and \textbf that
            generate combined markups.
            <br/><br/>
            <div style={{margin: "0 auto", textAlign: "center"}}>
                <figure>
                    <Zoom>
                        <img
                            alt="bounding boxes"
                            src="../projects_image_to_latex_04.jpg"
                            width="100%"
                        />
                    </Zoom>
                    <figcaption style={{fontSize: "15px"}}><em>
                        Sample identified bounding boxes in the images
                    </em></figcaption>
                </figure>
            </div>
            <br/><br/>
            <div className="heading-1">Assigning a Vector for Each Markup</div>
            <br/><br/>
            The objective in this section is to assign a compact vector to each of the 2608 unique markups. These
            vectors will be utilized in the subsequent pre-training of the CNN network.
            <br/><br/>
            The basic idea involves using dimensionality reduction tools, such as PCA, to condense the pixel space to a
            few numbers. However, it is crucial to note that the distribution of pixels within the markups does not
            adhere to a normal distribution. In reality, markups are grouped together, with some highly correlated
            within their respective group but distant from other correlated groups. Consequently, PCA is not the most
            suitable approach for this purpose.
            <br/><br/>
            For this purpose, I employed a novel approach. Initially, I utilized KMeans to segregate highly correlated
            images into different clusters. Subsequently, I applied PCA independently in each cluster. The final vector
            for each markup is generated by concatenating two vectors. The first component is the center of the
            corresponding cluster, reduced to 5 dimensions by applying PCA to all central points. The second component
            is the 11-dimensional vector obtained by applying PCA to the markup within its cluster. Consequently, each
            16-dimensional vector for a markup encapsulates two key aspects: 1) the cluster to which the markup belongs,
            and 2) the precise location of the markup within that cluster.
            <br/><br/>
            <div style={{margin: "0 auto", textAlign: "center"}}>
                <figure>
                    <Zoom>
                        <img
                            alt="pca markup"
                            src="../projects_image_to_latex_05.png"
                            width="100%"
                        />
                    </Zoom>
                    <figcaption style={{fontSize: "15px"}}><em>
                        Each markup is represented by a dot in a two-dimensional space, with each dot corresponding to
                        the assigned vector for the respective markup.
                    </em></figcaption>
                </figure>
            </div>
            <br/><br/>
            <div className="heading-1">Training CNN Network</div>
            <br/><br/>
            As a recap, our goal is to implement an attention mechanism to address two key questions: 1. What markups
            constitute the given image? 2. Where is the location of each markup within the image? With the completion of
            the preceding steps, we have already addressed these questions. The 16-dimensional vector assigned to each
            markup represents the markup itself, while the bounding box identified for each markup indicates its
            location. With this information, we are now ready to proceed with the pre-training of our CNN network.
            <br/><br/>
            I utilized inception modules at different layers of the network because this architecture is well-suited for
            handling different sizes of input images. Given that distinct markups are presented in the input image with
            varying sizes, the network needs to be capable of effectively capturing these differences.
            <br/><br/>
            I created mock labels for training this network. Assuming the input image size is 60x360, I generated a
            label map with dimensions 60x360x16. Let's take the example of the markup 'A' appearing in the input image
            with a bounding box from point [30, 110] to point [40, 120]. The label map in this specified rectangle is
            filled with the vector assigned to markup 'A.' This process is iteratively applied to all markups in the
            image. Then, the network is trained to predict this label map.
            <br/><br/>
            <div style={{margin: "0 auto", textAlign: "center"}}>
                <figure>
                    <Zoom>
                        <img
                            alt="sample BB"
                            src="../projects_image_to_latex_06.jpg"
                            width="100%"
                        />
                    </Zoom>
                    <figcaption style={{fontSize: "15px"}}><em>
                        The sample image is displayed at the top, accompanied by its mock label map on the left and the
                        CNN network's prediction on the right. Each row in the label map and prediction corresponds to a
                        specific third dimension (16 layers overall).
                    </em></figcaption>
                </figure>
            </div>
            <br/><br/>
            By employing this approach, we ensure that the CNN network is pre-trained to attend to various locations
            within the image. The network is now prepared to be integrated into the attention mechanism to extract the
            LaTeX code.
            <br/><br/>
            <div className="heading-1">Transformer</div>
            <br/><br/>
            In the final step, a language model is employed to translate these markups considering their respective
            locations in the image. The Transformer architecture, known for its reliance on the attention mechanism, is
            chosen for this task. The extracted feature maps from the CNN inception network serve as the input for the
            Transformer, which sequentially generates each vocabulary at every time step.
            <br/><br/>
            In the embedding section of the Transformer, I employed the skip-gram method to pre-train the embedding
            matrix. Additionally, certain parts of the network, notably the positional encoding block, were modified to
            accommodate three-dimensional images. The provided image illustrates a sample input image along with its
            predicted LaTeX code.
            <br/><br/>
            <div style={{margin: "0 auto", textAlign: "center"}}>
                <figure>
                    <Zoom>
                        <img
                            alt="transformer results"
                            src="../projects_image_to_latex_07.png"
                            width="100%"
                        />
                    </Zoom>
                    <figcaption style={{fontSize: "15px"}}><em>
                        Results of Transformer network with attention to different locations for generating markups.
                    </em></figcaption>
                </figure>
            </div>
            <br/><br/>
            <div className="heading-1">Conclusion</div>
            <br/><br/>
            This article addresses the difficulty of integrating mathematical equations into LaTeX papers and proposes a
            solution using deep NNs with attention mechanisms. I modified the structure to enhance attention and
            suggests using Inception modules in CNN blocks. I presented challenges in handling unique vocabularies. The
            process involves finding bounding boxes, assigning vectors to markups, and training a CNN network. The final
            step employs a Transformer architecture to translate markups into LaTeX code based on their locations in the
            image.
            <br/><br/>
            In my opinion, all steps were executed successfully except for the final one. The Transformer network did
            not generate the codes as anticipated. For future work, readers are encouraged to propose improved
            structures to enhance the performance of the Transformer network.
        </div>
    );
};


const ProjectBody03 = () => {
    return (
        <div>
            BUILDING ...
        </div>
    );
};

const ProjectBodies = [ProjectBody01, ProjectBody02, ProjectBody03];
export default ProjectBodies;
